{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook is showcases the process of building an NLP Topic Model using `Latent Dirichlet Allocation` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## installing required libraries\n",
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install plotly\n",
    "# ! pip install nbformat\n",
    "# ! pip install ipykernel\n",
    "# ! pip install matplotlip\n",
    "# ! pip install wordcloud\n",
    "# ! pip install gensim\n",
    "# ! pip install pyLDAvis\n",
    "# ! pip install nltk\n",
    "# ! pip install spacy\n",
    "# !python -m spacy download en_core_web_lg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaurang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>soft_title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_name</th>\n",
       "      <th>word_count</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.foxnews.com/politics/biden-says-xi...</td>\n",
       "      <td>Greg Norman</td>\n",
       "      <td>2022-11-14 00:00:00+00:00</td>\n",
       "      <td>Biden says after Xi meeting he doesn’t believe...</td>\n",
       "      <td>Biden says after Xi meeting he doesn’t believe...</td>\n",
       "      <td>President Biden said following his meeting wit...</td>\n",
       "      <td>President Biden told reporters Monday followin...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>Nov</td>\n",
       "      <td>356</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.foxnews.com/politics/gop-rep-calve...</td>\n",
       "      <td>Sophia Slacik</td>\n",
       "      <td>2022-11-14 00:00:00+00:00</td>\n",
       "      <td>GOP Rep. Calvert wins election in competitive ...</td>\n",
       "      <td>GOP Rep. Calvert wins election in competitive ...</td>\n",
       "      <td>The race for California 41st House district, o...</td>\n",
       "      <td>The Associated Press projects that GOP Rep. Ke...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>Nov</td>\n",
       "      <td>228</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.foxnews.com/politics/pelosi-not-ev...</td>\n",
       "      <td>Haris Alic</td>\n",
       "      <td>2022-11-14 00:00:00+00:00</td>\n",
       "      <td>Pelosi 'not even thinking' about political fut...</td>\n",
       "      <td>Pelosi 'not even thinking' about political fut...</td>\n",
       "      <td>House Speaker Nancy Pelosi’s spokesman said th...</td>\n",
       "      <td>House Speaker Nancy Pelosi’s spokesman forcefu...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>Nov</td>\n",
       "      <td>334</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.foxnews.com/politics/arizona-gover...</td>\n",
       "      <td>Paul Steinhauser</td>\n",
       "      <td>2022-10-25 00:00:00+00:00</td>\n",
       "      <td>Katie Hobbs defeats GOP challenger Kari Lake, ...</td>\n",
       "      <td>Arizona gov election: Katie Hobbs defeats GOP ...</td>\n",
       "      <td>Democratic Secretary of State Katie Hobbs has ...</td>\n",
       "      <td>The Fox News Decision Desk can project that De...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>Oct</td>\n",
       "      <td>266</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.foxnews.com/us/idaho-quadruple-hom...</td>\n",
       "      <td>Paul Best</td>\n",
       "      <td>2022-11-14 00:00:00+00:00</td>\n",
       "      <td>'Crime of passion,' 'burglary gone wrong' amon...</td>\n",
       "      <td>Idaho quadruple student homicide: 'Crime of pa...</td>\n",
       "      <td>Idaho police are trying to narrow down a motiv...</td>\n",
       "      <td>Four college students were killed around 3:00 ...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>2022</td>\n",
       "      <td>Nov</td>\n",
       "      <td>518</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url            author                       date                                              title                                         soft_title                                        description                                               text  day  month  year month_name  word_count  line_count\n",
       "0  https://www.foxnews.com/politics/biden-says-xi...       Greg Norman  2022-11-14 00:00:00+00:00  Biden says after Xi meeting he doesn’t believe...  Biden says after Xi meeting he doesn’t believe...  President Biden said following his meeting wit...  President Biden told reporters Monday followin...   14     11  2022        Nov         356          17\n",
       "1  https://www.foxnews.com/politics/gop-rep-calve...     Sophia Slacik  2022-11-14 00:00:00+00:00  GOP Rep. Calvert wins election in competitive ...  GOP Rep. Calvert wins election in competitive ...  The race for California 41st House district, o...  The Associated Press projects that GOP Rep. Ke...   14     11  2022        Nov         228           9\n",
       "2  https://www.foxnews.com/politics/pelosi-not-ev...        Haris Alic  2022-11-14 00:00:00+00:00  Pelosi 'not even thinking' about political fut...  Pelosi 'not even thinking' about political fut...  House Speaker Nancy Pelosi’s spokesman said th...  House Speaker Nancy Pelosi’s spokesman forcefu...   14     11  2022        Nov         334          19\n",
       "3  https://www.foxnews.com/politics/arizona-gover...  Paul Steinhauser  2022-10-25 00:00:00+00:00  Katie Hobbs defeats GOP challenger Kari Lake, ...  Arizona gov election: Katie Hobbs defeats GOP ...  Democratic Secretary of State Katie Hobbs has ...  The Fox News Decision Desk can project that De...   25     10  2022        Oct         266          12\n",
       "4  https://www.foxnews.com/us/idaho-quadruple-hom...         Paul Best  2022-11-14 00:00:00+00:00  'Crime of passion,' 'burglary gone wrong' amon...  Idaho quadruple student homicide: 'Crime of pa...  Idaho police are trying to narrow down a motiv...  Four college students were killed around 3:00 ...   14     11  2022        Nov         518          21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading scrapped data from data/apify_dataset_clean.csv\n",
    "data = pd.read_csv('../data/apify_dataset_clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* This data has been scrapped using the `apify` application and we've performed initial feature engineering and `EDA` in `eda_apify_data.ipynb` notebook. \n",
    "* For the purposes of EDA we are only interested in `text` column, which contains text from news articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elvis Presley, The king of rock \\'n\\' roll, capped the most extraordinary breakout year in pop-culture history with the release of his first movie on this day in history, Nov. 15, 1956.\\n\\n\"Love Me Tender\" — and Elvis the actor — garnered only tepid reviews. But the film helped turn the groundbreaking recording star into a multimedia icon who is still to this day beloved around the world, 45 years after his death at age 42.\\n\\n\"Appraising Presley as an actor, he ain’t,\" Variety magazine wrote of the movie at the time.\\n\\nON THIS DAY IN HISTORY, NOV. 14, 1776, BRITISH PRESS NAMES FAMOUS LONDONER BEN FRANKLIN LEADER OF REBELLION\\n\\n\"Not that it makes much difference. There are four songs, and lotsa Presley wriggles thrown in for good measure.\"\\n\\n\"Love Me Tender\" debuted amid great fanfare at the Paramount Theatre in Times Square in New York City.\\n\\nPresley, just 21 at the time, starred as Clint Reno, a man caught in a love triangle with his Confederate veteran brother in the Civil War-era western.\\n\\nElvis would go on to star in 30 more feature films.\\n\\n\"For a number of years he was one of Hollywood’s top box office draws and one of its highest-paid actors,\" says the website of Graceland, Presley\\'s Memphis home.\\n\\n‘A CHRISTMAS STORY’ HOUSE IN OHIO LISTED FOR SALE JUST IN TIME FOR CHRISTMAS: ‘DEFINITELY EXCITING’\\n\\n\"The first Elvis film, released in 1956, was followed by his two most critically acclaimed films, ‘Jailhouse Rock\\' (1957) and ‘King Creole’ (1958), which have become classics of their era.\"\\n\\nPresley exploded onto the American and then global pop-culture scene in 1956 in a glittering meteoric rise experienced by no other artist before or since.\\n\\n\"Appraising Presley as an actor, he ain’t.\" — Variety magazine\\n\\nHe released an incredible five songs to top the Billboard pop charts that year, with \"Heartbreak Hotel,\" \"I Want You, I Need You, I Love You,\" \"Don\\'t Be Cruel,\" \"Hound Dog\" and movie title tune \"Love Me Tender.\"\\n\\nElvis spent nearly half of 1956 — 25 weeks — with a No. 1 Billboard song.\\n\\nIn an extraordinarily rare example of crossover appeal, \"Don\\'t Be Cruel\" and \"Hound Dog\" also topped both the country and R&B charts.\\n\\nElvis placed 12 other songs in the Billboard 100 that year, while performing 143 concerts in 79 cities, according to fan site Elvis History Blog.\\n\\nELVIS PRESLEY\\'S FINAL MONTHS WERE PLAGUED WITH PHYSICAL PAIN AS HE EMBARKED ON GRUELING TOUR, AUTHOR CLAIMS\\n\\nElvis, one of the few stars in history commonly known only by his first name, also dazzled on the biggest TV programs in the nation for the first time that same year.\\n\\n\"Elvis spent nearly half of 1956 — 25 weeks — with a No. 1 Billboard song.\"\\n\\n\"In 1956, Elvis Presley made his television debut with six appearances on ‘Stage Show,’ two appearances on ‘The Milton Berle Show,’\\xa0one on the\\xa0‘The Steve Allen Show,’ in which he sang one of his newly recorded songs (\\'Hound Dog\\') and performed in a comedy sketch before making two appearances on Allen’s competitor ’The Ed Sullivan Show,\\'\" reports Graceland.\\n\\nCLICK HERE TO GET THE FOX NEWS APP\\n\\nBut radio and recording is where the Elvis star shined brightest.\\n\\nHe\\'d released a handful of minor hits in the previous two years, \"That\\'s All Right, Mama,\" most notably in 1954.\\n\\nHe burst onto the national scene with his eponymous first studio rock ‘n’ roll album \"Elvis Presley\" in March 1956.\\n\\nIt \"went on to become the first rock and roll album to reach number one on national record sales charts, and RCA\\'s first million-dollar-earning pop album,\" writes the National Museum of American History.\\n\\nElvis in 1956 turned rock ‘n’ roll from an underground music form, performed largely by black artists, into a major pop-culture phenomenon around the world.\\n\\nCLICK HERE TO SIGN UP FOR OUR LIFESTYLE NEWSLETTER\\n\\nMovie review site Rotten Tomatoes says in its Presley biography, \"His success and magnetism inspired a host of future musicians, including Bob Dylan, John Lennon and others, to take their first steps towards careers in rock and roll.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets look at sample data\n",
    "data.loc[15, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions for Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'from', 'subject', 're', 'edu', 'use', 'say', 'one', 'time', 'people', 'know', 'like', 'tell', 'get', 'year', 'go', 'around', 'award', 'actually', 'carry', 'new', 'it', 'show', 'news', 'go', 'fox', 'make', 'do', 'not', 'say', 'also', 'love', 'it', 'star', 'go', 'do', 'say', 'not', 'said']\n"
     ]
    }
   ],
   "source": [
    "# lets break down the cleaning functions into smaller functions\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'say', 'one', 'time', 'people',\n",
    "                  'know', 'like', 'tell', 'get', 'year', 'go', 'around', 'award', 'actually', 'carry',\n",
    "                   'new', 'it', 'show', 'news', 'go', 'fox', 'make', 'do', 'not', 'say',\n",
    "                   'also', 'love', 'it', 'star', 'go', 'do', 'say', 'not', 'said'\n",
    "                   ])\n",
    "print(stop_words)\n",
    "# function to clean html tags from text\n",
    "\n",
    "\n",
    "def clean_html(html):\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "# function to convert text to lowercase\n",
    "\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# function to remove line breaks\n",
    "\n",
    "\n",
    "def remove_line_breaks(text):\n",
    "    return re.sub(r'\\n', '', text)\n",
    "\n",
    "# function to remove punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# function to remove numbers\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# function to remove extra spaces\n",
    "\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "# function to remove stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# function for text lemmatization using spacy\n",
    "\n",
    "\n",
    "def lemmatize_text(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to clean text\n",
    "def clean_text(text):     \n",
    "     ## clean html tags\n",
    "     text = clean_html(text)\n",
    "     \n",
    "     ## convert text to lowercase\n",
    "     text = lower_case(text)\n",
    "     \n",
    "     ## remove line breaks\n",
    "     text = remove_line_breaks(text)\n",
    "     \n",
    "     ## remove extra spaces\n",
    "     text = remove_extra_spaces(text)\n",
    "     \n",
    "     ## remove punctuation\n",
    "     text = remove_punctuation(text)\n",
    "     \n",
    "     ## remove numbers\n",
    "     text = remove_numbers(text)\n",
    "     \n",
    "     ## lemmatize text\n",
    "     text = lemmatize_text(text)\n",
    "     \n",
    "     ## remove stopwords\n",
    "     text = remove_stopwords(text)\n",
    "     \n",
    "     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'king rock n roll cap extraordinary breakout popculture history release first movie day history tender actor garner tepid review film help turn groundbreaking recording multimedia icon still day belove world death age appraise presley actor variety magazine write movie timeon day history british press name famous much difference song wriggle throw good tender debut great fanfare paramount theatre square man catch triangle confederate veteran brother civil warera feature filmsfor number top box office draw highestpaid actor website list sale definitely film release follow critically acclaim film jailhouse rock king creole become classic erapresley explode american global popculture scene glitter meteoric rise experience artist sinceappraise presley actor ai variety magazinehe release incredible song top billboard pop chart heartbreak hotel want need cruel hound dog movie title tune spend nearly half week billboard songin extraordinarily rare example crossover appeal cruel hound dog top country rb chartselvis place song billboard perform concert city accord history blogelvis final month plague physical pain embark gruele tour author claimselvi history commonly first name dazzle big tv program nation first spend nearly half week billboard songin television debut appearance stage appearance sing newly record song dog perform comedy sketch appearance competitor report radio recording shine brightesthe release handful minor hit previous right mama notably burst national scene eponymous first studio rock roll album become first rock roll album reach number national record sale chart rca first milliondollarearne pop album write turn rock roll underground music form perform largely black artist major popculture phenomenon sign lifestyle newslettermovie review site rotten tomato presley biography success magnetism inspire host future musician include take first step career rock roll'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(data.loc[15, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So the text `\"looks clean\"`, we can run this for our dataset and revisit cleaning if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "* Am not sure if we need to split train/test. But I would like to train the model on some data and then test it on new data to see if it assigns the right topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Set Shape :  (7702, 13)\n",
      "Testing Data Set Shape :  (3301, 13)\n"
     ]
    }
   ],
   "source": [
    "## verifing the shape\n",
    "print(\"Training Data Set Shape : \", train.shape)\n",
    "print(\"Testing Data Set Shape : \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets apply the cleaning function to all the text\n",
    "train[\"clean_text\"] = train[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to creation tfidf matrix for the text\n",
    "def create_tfidf_matrix(data, max_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "    return tfidf_matrix, tfidf_vectorizer\n",
    "\n",
    "\n",
    "vect_text, text_vectorizer = create_tfidf_matrix(train[\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn LDA with TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online') \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: \n",
      "Topic  0 :  89.12991282657808 %\n",
      "Topic  1 :  1.2080404657366075 %\n",
      "Topic  2 :  1.2077345739199736 %\n",
      "Topic  3 :  1.2079045006761884 %\n",
      "Topic  4 :  1.207601565055684 %\n",
      "Topic  5 :  1.207901736363814 %\n",
      "Topic  6 :  1.2076836330464362 %\n",
      "Topic  7 :  1.2076015092615067 %\n",
      "Topic  8 :  1.2079162179135658 %\n",
      "Topic  9 :  1.2077029714481455 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['government', 'climate', 'ukrainian', 'country', 'official', 'military', 'war', 'russian', 'migrant', 'border']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['build', 'front', 'available', 'drive', 'sell', 'system', 'price', 'model', 'vehicle', 'car']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['district', 'teach', 'gender', 'theory', 'child', 'education', 'teacher', 'parent', 'student', 'school']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['assault', 'investigation', 'court', 'woman', 'report', 'arrest', 'crime', 'charge', 'officer', 'police']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['spending', 'team', 'season', 'veteran', 'model', 'hotel', 'democracy', 'legislation', 'car', 'coach']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['work', 'good', 'want', 'first', 'take', 'family', 'day', 'share', 'life', 'think']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['disease', 'mask', 'drug', 'study', 'test', 'health', 'fentanyl', 'virus', 'vaccine', 'covid']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['earn', 'voter', 'committee', 'master', 'record', 'label', 'version', 'song', 'album', 'swift']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['democratic', 'race', 'campaign', 'candidate', 'trump', 'state', 'abortion', 'voter', 'vote', 'election']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['democracy', 'lifestyle', 'content', 'speech', 'company', 'purchase', 'free', 'platform', 'twitter', 'musk']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## analyzing the topics\n",
    "vocab = text_vectorizer.get_feature_names_out()\n",
    "\n",
    "## lets create a function that will return the top 10 words for given topic number\n",
    "def print_topic_words(topic_number, model, vocab):\n",
    "    print(f'Top 10 words for topic #{topic_number}:')\n",
    "    print([vocab[i] for i in model.components_[topic_number].argsort()[-10:]])\n",
    "    print('\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    print_topic_words(i, lda_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets create a function that would return topics for given document\n",
    "def print_document_topics(document_number):\n",
    "    print(f'Topics for document #{document_number}:')\n",
    "    for i,topic in enumerate(lda_top[document_number]):\n",
    "        print(\"Topic \",i,\": \",topic*100,\"%\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  1529.0185103407616\n",
      "Log Likelihood: -414558.7954475313\n"
     ]
    }
   ],
   "source": [
    "## Analyzing the model performance. \n",
    "print(\"Perplexity: \", lda_model.perplexity(vect_text))\n",
    "# print(\"Coherence Score: \", CoherenceModel(model=lda_model, texts=train[\"clean_text\"], dictionary=dictionary, coherence='c_v').get_coherence())\n",
    "print(\"Log Likelihood:\", lda_model.score(vect_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
       "                         &#x27;n_components&#x27;: [5, 10, 15, 20, 25, 30]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
       "                         &#x27;n_components&#x27;: [5, 10, 15, 20, 25, 30]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [5, 10, 15, 20, 25, 30]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets do grid search for the best parameters\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -87141.47510331546\n",
      "Model Perplexity:  1312.340644544248\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(vect_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Best Model Params\n",
    "```\n",
    "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 5}\n",
    "Best Log Likelihood Score:  -87178.7585565557\n",
    "Model Perplexity:  1290.9544364071992\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda = best_lda_model.transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: \n",
      "Topic  0 :  2.484731056650414 %\n",
      "Topic  1 :  2.5838521898290967 %\n",
      "Topic  2 :  2.9790407036326254 %\n",
      "Topic  3 :  2.529371464836205 %\n",
      "Topic  4 :  89.42300458505167 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(best_lda[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['democratic', 'campaign', 'trump', 'candidate', 'border', 'state', 'abortion', 'voter', 'vote', 'election']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['swift', 'game', 'woman', 'report', 'man', 'accord', 'team', 'charge', 'police', 'car']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['country', 'force', 'war', 'city', 'official', 'military', 'police', 'report', 'russian', 'school']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['thing', 'really', 'film', 'feel', 'child', 'want', 'share', 'family', 'life', 'think']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['inflation', 'price', 'company', 'gas', 'food', 'musk', 'energy', 'recipe', 'oil', 'climate']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_topic_words(i, best_lda_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_matrix, tf_vectorizer = create_tfidf_matrix(test.sample(1)[\"clean_text\"], max_features=1000)\n",
    "# topics = lda_model.fit_transform(tf_matrix)\n",
    "# topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA with BOW Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA2Vec with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "072e000fc02283ef03ef016ae4d471af267a655ab38f930dab1b1ce7ec9f12c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
