{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook is showcases the process of building an NLP Topic Model using `Latent Dirichlet Allocation` method. \n",
    "* The dataset we are going to use are `text` and `soft text` from `scrapped_fox_data_clean.csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
      "     --------------------------------------- 42.8/42.8 MB 20.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from en-core-web-md==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (63.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.13)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\aletheia\\env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\workspace\\aletheia\\env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.4.1\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in d:\\workspace\\aletheia\\env\\lib\\site-packages (1.2.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## installing required libraries\n",
    "# ! pip install beautifulsoup4\n",
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install plotly\n",
    "# ! pip install nbformat\n",
    "# ! pip install ipykernel\n",
    "# ! pip install matplotlip\n",
    "# ! pip install wordcloud\n",
    "# ! pip install gensim\n",
    "# ! pip install pyLDAvis\n",
    "# ! pip install nltk\n",
    "# ! pip install -U pip setuptools wheel\n",
    "# ! pip install -U spacy\n",
    "# ! python -m spacy download en_core_web_trf \n",
    "! python -m spacy download en_core_web_md\n",
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "d:\\workspace\\Aletheia\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\workspace\\Aletheia\\env\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as io\n",
    "\n",
    "# loading library\n",
    "import pickle\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3972, 12)\n"
     ]
    }
   ],
   "source": [
    "## reading manaully scrapped data\n",
    "data = pd.read_csv('../data/scrapped_fox_data_clean.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "## extending stopwords\n",
    "# lets break down the cleaning functions into smaller functions\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "## trying to remove stopwords from stopwords super set. \n",
    "stopwords_super_set = pd.read_csv(\"../data/stopwords/sw1k.csv\")\n",
    "\n",
    "## filtering stopwords to pronouns and other type\n",
    "stopwords_to_remove = list(stopwords_super_set.loc[(stopwords_super_set[\"type\"] == \"G\" ), \"term\"])\n",
    "\n",
    "\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'say', 'one', 'time', 'people',\n",
    "#                   'know', 'like', 'tell', 'get', 'year', 'go', 'around', 'award', 'actually', 'carry',\n",
    "#                    'new', 'it', 'show', 'news', 'go', 'fox', 'make', 'do', 'not', 'say',\n",
    "#                    'also', 'love', 'it', 'star', 'go', 'do', 'say', 'not', 'said'\n",
    "#                    ])\n",
    "\n",
    "# stop_words.extend(stopwords_to_remove)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')\n",
    "# nlp.add_pipe('merge_entities')\n",
    "# nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "# Utility Functions for Text Cleaning\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# function to clean html tags from text\n",
    "\n",
    "\n",
    "def clean_html(html):\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "# function to convert text to lowercase\n",
    "\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# function to remove line breaks\n",
    "\n",
    "\n",
    "def remove_line_breaks(text):\n",
    "    return re.sub(r'\\n', '', text)\n",
    "\n",
    "# function to remove punctuation\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# function to remove numbers\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# function to remove extra spaces\n",
    "\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "# function to remove stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    preprocess_text = simple_preprocess(str(texts), deacc=True)\n",
    "    word_list = [word for word in preprocess_text if word not in stop_words]\n",
    "    return \" \".join(word_list)\n",
    "    # return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# helper function to create pos tags\n",
    "\n",
    "\n",
    "def create_pos_tag(str_sent):\n",
    "    return nlp(str_sent)\n",
    "\n",
    "# function for text lemmatization using spac\n",
    "##'ADJ', 'VERB'\n",
    "def lemmatization(texts, allowed_postags=['PROPN', 'NOUN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [\"_\".join(token.lemma_.split(\" \")) for token in doc if (token.pos_ in allowed_postags and token.is_alpha and token.is_stop == False)])\n",
    "    return texts_out\n",
    "\n",
    "def lemmatization_without_pos(texts):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "## helper function to create pos tags distribution\n",
    "def create_pos_tags_distribution(docs = []):\n",
    "    token_distribution = {}\n",
    "    is_alpha = 0\n",
    "    is_stop = 0\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            token_distribution[token.pos_] = token_distribution.get(token.pos_, 0) + 1\n",
    "            if(token.is_alpha):\n",
    "                is_alpha += 1\n",
    "            if(token.is_stop):\n",
    "                is_stop += 1\n",
    "    return token_distribution, is_alpha, is_stop\n",
    "\n",
    "\n",
    "# function to create n-grams from noun chunks\n",
    "def create_noun_chunk_ngrams(docs):\n",
    "    n_gram_docs = []\n",
    "    for doc in docs:\n",
    "        doc_text = doc.text\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_n_gram = \"_\".join(chunk.text.split(\" \"))\n",
    "            doc_text = doc_text.replace(chunk.text, chunk_n_gram)\n",
    "        n_gram_docs.append(doc_text.split(\" \"))\n",
    "    return n_gram_docs\n",
    "\n",
    "\n",
    "def lemmatization_noun_chunks(texts):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if (\n",
    "            (\"_\" in token.text) or ## if the token is a noun chunk allow that\n",
    "            (token.pos_ in ['NOUN', 'PROPN'] and token.is_alpha and token.is_stop == False) ## if the token is a noun or proper noun allow that\n",
    "        )])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Models Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to compute optimal parameters for LDA model\n",
    "def compute_coherence_values(dictionary, corpus, id2word, texts, num_topics, passes, chunk_sizes=[200]):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    params = []\n",
    "    for num_topic in num_topics:\n",
    "        for chunk_size in chunk_sizes:\n",
    "            for num_passes in passes:\n",
    "                model = LdaModel(corpus=corpus,\n",
    "                                 id2word=id2word,\n",
    "                                 num_topics=num_topic,\n",
    "                                 random_state=100,\n",
    "                                 update_every=1,\n",
    "                                 chunksize=chunk_size,\n",
    "                                 passes=num_passes,\n",
    "                                 per_word_topics=True)\n",
    "                model_list.append(model)\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_lda = coherencemodel.get_coherence()\n",
    "                coherence_values.append(coherence_lda)\n",
    "                params.append({'num_topics': num_topic, 'chunk_size': chunk_size, 'passes': num_passes})\n",
    "\n",
    "    return model_list, coherence_values, params\n",
    "\n",
    "def analyze_gensim_lda_model(lda_model, corpus, id2word, texts, num_topics, passes, chunk_sizes=[200]):\n",
    "    # Compute Perplexity\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## helper functions to visualize LDA model\n",
    "def visualize_gensim_lda_model(lda_model, corpus, id2word, filename=\"gensim_lda.html\"):\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    vis.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Model Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "\n",
    "def print_sklearn_sparcity(data_vectorized):\n",
    "    # Materialize the sparse data\n",
    "    data_dense = data_vectorized.todense()\n",
    "\n",
    "    # Compute Sparsicity = Percentage of Non-Zero cells\n",
    "    print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "\n",
    "def create_sklearn_dominent_topic_dataframe(lda_model, data_vectorized):\n",
    "    lda_output = lda_model.transform(data_vectorized)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "    \n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "    return df_document_topic\n",
    "\n",
    "def print_sklearn_dominant_topics(lda_model, data_vectorized):\n",
    "    df_document_topic = create_sklearn_dominent_topic_dataframe(lda_model, data_vectorized)\n",
    "    # Apply Style\n",
    "    df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "    return df_document_topics\n",
    "\n",
    "def print_sklearn_topic_distribution(lda_model, data_vectorized):\n",
    "    df_document_topic = create_sklearn_dominent_topic_dataframe(lda_model, data_vectorized)\n",
    "    df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\").rename(columns={'index':'Topic'})\n",
    "    # df_topic_distribution.columns = [\"Topic Num\", \"Num Documents\"]\n",
    "    return df_topic_distribution\n",
    "\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_sklearn_topics(vectorizer, lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names_out())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "def format_sklearn_topics(topic_keywords):\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    return df_topic_keywords\n",
    "\n",
    "def analyze_sklearn_lda_model(lda_model, data_vectorized):\n",
    "    # Log Likelyhood: Higher the better\n",
    "    print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "## helper function to visualize lda model\n",
    "def visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer, mds='tsne'):    \n",
    "    pyLDAvis.enable_notebook()\n",
    "    panel2 = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds=mds)\n",
    "    return panel2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "     text = clean_html(text)\n",
    "     text = lower_case(text)\n",
    "     text = remove_line_breaks(text)\n",
    "     text = remove_punctuation(text)\n",
    "     text = remove_numbers(text)\n",
    "     text = remove_extra_spaces(text)\n",
    "     text = remove_stopwords(text)\n",
    "     return text\n",
    "\n",
    "data[\"clean_text\"] = data[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       former governor first term democratic sen magg...\n",
       "1       president biden urged democrats wednesday show...\n",
       "2       famous naked cowboy new york citys times squar...\n",
       "3       liberal groups wisconsin seeking change rules ...\n",
       "4       texas gubernatorial nominee beto rourke among ...\n",
       "                              ...                        \n",
       "3967    former vice president mike pence said monday a...\n",
       "3968    house senate office building anywhere us capit...\n",
       "3969    first fox gov ron desantis reelection campaign...\n",
       "3970    first fox new documents exclusively obtained f...\n",
       "3971    runs year reelection amid difficult political ...\n",
       "Name: clean_text, Length: 3972, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data['clean_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check for duplicates\n",
    "data[\"clean_text\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3972.000000\n",
       "mean      356.933535\n",
       "std       193.554195\n",
       "min        20.000000\n",
       "25%       233.000000\n",
       "50%       315.000000\n",
       "75%       429.000000\n",
       "max      5011.000000\n",
       "Name: text_word_count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_word_count'] = data['clean_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "data['text_word_count'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\Aletheia\\env\\lib\\site-packages\\plotly\\io\\_renderers.py:395: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "d:\\workspace\\Aletheia\\env\\lib\\site-packages\\plotly\\io\\_renderers.py:395: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "text_word_count=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "type": "histogram",
         "x": [
          704,
          279,
          138,
          228,
          186,
          203,
          340,
          188,
          507,
          439,
          295,
          334,
          395,
          728,
          527,
          516,
          229,
          268,
          243,
          936,
          151,
          232,
          451,
          337,
          517,
          207,
          181,
          247,
          192,
          346,
          342,
          236,
          611,
          188,
          271,
          310,
          357,
          195,
          192,
          220,
          331,
          187,
          486,
          167,
          533,
          498,
          192,
          156,
          266,
          236,
          343,
          649,
          382,
          414,
          272,
          495,
          195,
          289,
          306,
          218,
          628,
          480,
          229,
          632,
          113,
          368,
          198,
          140,
          109,
          647,
          394,
          235,
          267,
          338,
          351,
          391,
          597,
          286,
          211,
          202,
          250,
          204,
          226,
          365,
          465,
          151,
          337,
          252,
          995,
          379,
          158,
          342,
          185,
          383,
          137,
          374,
          448,
          229,
          534,
          167,
          228,
          1372,
          171,
          263,
          279,
          348,
          240,
          221,
          689,
          339,
          215,
          250,
          268,
          147,
          340,
          292,
          167,
          671,
          199,
          441,
          164,
          275,
          459,
          412,
          253,
          275,
          238,
          218,
          297,
          130,
          240,
          216,
          518,
          487,
          785,
          482,
          438,
          320,
          195,
          249,
          341,
          436,
          197,
          317,
          189,
          350,
          453,
          143,
          417,
          198,
          343,
          160,
          187,
          114,
          266,
          238,
          278,
          311,
          1290,
          272,
          579,
          449,
          319,
          309,
          371,
          281,
          425,
          312,
          262,
          225,
          294,
          241,
          197,
          450,
          243,
          624,
          286,
          221,
          304,
          164,
          304,
          303,
          398,
          138,
          394,
          306,
          180,
          206,
          309,
          471,
          668,
          281,
          283,
          156,
          319,
          211,
          986,
          253,
          508,
          210,
          237,
          361,
          173,
          355,
          308,
          297,
          265,
          748,
          313,
          501,
          360,
          653,
          211,
          815,
          325,
          473,
          196,
          521,
          168,
          220,
          284,
          869,
          415,
          452,
          375,
          438,
          371,
          347,
          622,
          228,
          137,
          214,
          538,
          376,
          239,
          626,
          375,
          449,
          488,
          185,
          189,
          175,
          345,
          315,
          357,
          78,
          204,
          235,
          264,
          327,
          333,
          200,
          347,
          249,
          236,
          576,
          488,
          306,
          280,
          524,
          198,
          261,
          413,
          237,
          292,
          238,
          231,
          352,
          329,
          548,
          571,
          398,
          110,
          455,
          240,
          186,
          306,
          286,
          611,
          282,
          294,
          236,
          529,
          257,
          340,
          294,
          169,
          340,
          344,
          310,
          111,
          335,
          287,
          475,
          255,
          1392,
          448,
          304,
          356,
          405,
          351,
          144,
          208,
          327,
          192,
          443,
          510,
          152,
          456,
          233,
          356,
          267,
          201,
          341,
          323,
          325,
          567,
          212,
          185,
          325,
          225,
          162,
          308,
          719,
          169,
          351,
          215,
          480,
          282,
          358,
          478,
          460,
          466,
          309,
          382,
          865,
          361,
          235,
          216,
          279,
          184,
          345,
          205,
          185,
          232,
          118,
          470,
          139,
          641,
          277,
          172,
          390,
          365,
          224,
          656,
          352,
          123,
          335,
          524,
          265,
          641,
          328,
          345,
          209,
          391,
          284,
          142,
          958,
          116,
          235,
          444,
          167,
          332,
          425,
          188,
          327,
          332,
          766,
          331,
          511,
          434,
          502,
          254,
          1048,
          314,
          199,
          200,
          465,
          175,
          206,
          364,
          463,
          293,
          544,
          422,
          430,
          392,
          245,
          248,
          162,
          282,
          273,
          510,
          198,
          197,
          319,
          265,
          305,
          343,
          301,
          289,
          201,
          779,
          315,
          338,
          309,
          289,
          604,
          428,
          342,
          314,
          330,
          193,
          365,
          325,
          268,
          364,
          210,
          200,
          266,
          252,
          797,
          273,
          234,
          350,
          249,
          286,
          549,
          227,
          271,
          498,
          906,
          211,
          325,
          247,
          351,
          256,
          307,
          394,
          161,
          260,
          198,
          462,
          186,
          176,
          214,
          333,
          207,
          290,
          208,
          639,
          377,
          290,
          345,
          322,
          161,
          428,
          355,
          524,
          562,
          760,
          258,
          523,
          226,
          358,
          163,
          430,
          576,
          582,
          298,
          241,
          234,
          208,
          392,
          359,
          183,
          295,
          221,
          441,
          543,
          276,
          811,
          169,
          293,
          295,
          227,
          338,
          203,
          251,
          100,
          344,
          257,
          183,
          680,
          197,
          334,
          149,
          336,
          289,
          405,
          175,
          188,
          232,
          585,
          265,
          126,
          149,
          229,
          196,
          560,
          372,
          580,
          62,
          540,
          167,
          219,
          301,
          332,
          377,
          423,
          784,
          215,
          234,
          368,
          456,
          465,
          187,
          201,
          526,
          297,
          171,
          543,
          270,
          493,
          270,
          315,
          443,
          240,
          332,
          278,
          593,
          243,
          228,
          358,
          172,
          770,
          192,
          228,
          221,
          324,
          315,
          275,
          346,
          541,
          297,
          551,
          681,
          171,
          627,
          161,
          225,
          224,
          407,
          197,
          573,
          561,
          5011,
          626,
          220,
          281,
          261,
          687,
          518,
          788,
          306,
          307,
          174,
          681,
          773,
          86,
          198,
          186,
          346,
          202,
          202,
          406,
          264,
          265,
          207,
          402,
          731,
          371,
          260,
          270,
          210,
          420,
          372,
          684,
          340,
          525,
          372,
          227,
          255,
          749,
          373,
          554,
          208,
          190,
          233,
          211,
          650,
          562,
          547,
          400,
          376,
          464,
          223,
          228,
          193,
          614,
          512,
          249,
          882,
          449,
          215,
          343,
          539,
          324,
          794,
          303,
          459,
          264,
          249,
          385,
          178,
          501,
          268,
          226,
          342,
          186,
          396,
          242,
          246,
          194,
          243,
          233,
          270,
          219,
          350,
          245,
          184,
          225,
          914,
          714,
          296,
          303,
          206,
          209,
          175,
          600,
          593,
          218,
          162,
          293,
          333,
          244,
          269,
          258,
          401,
          461,
          272,
          514,
          598,
          290,
          284,
          315,
          227,
          241,
          463,
          705,
          422,
          354,
          184,
          304,
          190,
          208,
          349,
          355,
          252,
          191,
          271,
          665,
          243,
          650,
          507,
          342,
          368,
          374,
          214,
          658,
          753,
          612,
          329,
          429,
          153,
          439,
          259,
          251,
          294,
          406,
          406,
          320,
          52,
          220,
          533,
          200,
          656,
          600,
          547,
          244,
          247,
          368,
          164,
          442,
          703,
          747,
          336,
          195,
          253,
          201,
          280,
          487,
          400,
          210,
          234,
          288,
          271,
          338,
          783,
          520,
          459,
          840,
          557,
          501,
          247,
          345,
          669,
          161,
          283,
          157,
          382,
          229,
          200,
          440,
          153,
          247,
          332,
          767,
          568,
          255,
          170,
          456,
          75,
          425,
          253,
          447,
          288,
          169,
          270,
          735,
          874,
          684,
          475,
          214,
          224,
          342,
          400,
          276,
          365,
          258,
          328,
          649,
          182,
          358,
          301,
          234,
          607,
          385,
          248,
          184,
          489,
          346,
          463,
          657,
          232,
          240,
          338,
          360,
          231,
          891,
          470,
          387,
          469,
          242,
          199,
          291,
          673,
          348,
          397,
          618,
          318,
          621,
          357,
          461,
          362,
          228,
          305,
          602,
          375,
          412,
          582,
          190,
          186,
          360,
          480,
          366,
          260,
          401,
          385,
          325,
          131,
          179,
          695,
          295,
          269,
          505,
          326,
          217,
          777,
          417,
          371,
          690,
          683,
          505,
          311,
          250,
          183,
          161,
          657,
          338,
          453,
          238,
          326,
          857,
          308,
          646,
          220,
          398,
          430,
          196,
          324,
          346,
          173,
          223,
          301,
          338,
          252,
          171,
          357,
          342,
          164,
          311,
          272,
          232,
          332,
          344,
          414,
          330,
          322,
          176,
          604,
          293,
          242,
          228,
          427,
          196,
          514,
          259,
          376,
          364,
          463,
          276,
          205,
          411,
          322,
          171,
          201,
          214,
          251,
          286,
          220,
          297,
          181,
          300,
          367,
          340,
          676,
          406,
          818,
          522,
          381,
          324,
          624,
          435,
          182,
          250,
          357,
          373,
          211,
          415,
          230,
          373,
          317,
          200,
          259,
          283,
          341,
          319,
          479,
          260,
          51,
          261,
          156,
          660,
          199,
          454,
          228,
          525,
          260,
          229,
          273,
          472,
          496,
          255,
          206,
          233,
          532,
          469,
          431,
          401,
          848,
          539,
          164,
          355,
          164,
          191,
          489,
          428,
          268,
          275,
          469,
          346,
          604,
          309,
          412,
          335,
          282,
          591,
          183,
          177,
          199,
          866,
          262,
          571,
          758,
          355,
          431,
          263,
          118,
          761,
          425,
          295,
          310,
          265,
          260,
          418,
          563,
          468,
          272,
          439,
          307,
          234,
          343,
          322,
          558,
          408,
          218,
          131,
          217,
          334,
          315,
          291,
          159,
          424,
          438,
          311,
          861,
          303,
          243,
          187,
          386,
          212,
          288,
          432,
          905,
          417,
          696,
          298,
          536,
          183,
          246,
          321,
          368,
          325,
          411,
          598,
          225,
          206,
          252,
          322,
          200,
          624,
          413,
          615,
          247,
          547,
          294,
          366,
          283,
          264,
          604,
          413,
          184,
          1015,
          825,
          366,
          297,
          205,
          134,
          138,
          904,
          79,
          223,
          397,
          589,
          343,
          488,
          453,
          163,
          375,
          251,
          320,
          526,
          228,
          266,
          184,
          236,
          539,
          445,
          527,
          311,
          202,
          328,
          746,
          539,
          249,
          959,
          195,
          171,
          193,
          145,
          234,
          178,
          301,
          290,
          304,
          267,
          524,
          719,
          825,
          242,
          239,
          739,
          434,
          397,
          427,
          231,
          831,
          404,
          515,
          232,
          170,
          201,
          645,
          244,
          251,
          428,
          199,
          199,
          368,
          534,
          298,
          197,
          266,
          252,
          276,
          335,
          767,
          330,
          311,
          269,
          316,
          571,
          209,
          419,
          501,
          360,
          598,
          683,
          482,
          608,
          214,
          542,
          278,
          374,
          330,
          848,
          228,
          307,
          228,
          171,
          287,
          206,
          325,
          183,
          222,
          336,
          259,
          430,
          207,
          713,
          296,
          609,
          298,
          399,
          198,
          214,
          258,
          285,
          331,
          148,
          389,
          456,
          412,
          192,
          849,
          311,
          783,
          495,
          332,
          188,
          305,
          271,
          263,
          124,
          126,
          550,
          357,
          368,
          189,
          628,
          254,
          518,
          429,
          477,
          282,
          321,
          223,
          470,
          475,
          446,
          330,
          163,
          209,
          345,
          263,
          707,
          194,
          328,
          252,
          849,
          482,
          302,
          325,
          147,
          251,
          287,
          343,
          306,
          239,
          160,
          511,
          241,
          340,
          305,
          496,
          368,
          440,
          761,
          226,
          536,
          681,
          482,
          627,
          201,
          411,
          294,
          284,
          326,
          350,
          760,
          339,
          352,
          553,
          378,
          351,
          324,
          339,
          205,
          286,
          420,
          494,
          235,
          237,
          288,
          329,
          317,
          319,
          409,
          415,
          1098,
          155,
          204,
          241,
          281,
          228,
          741,
          427,
          942,
          173,
          392,
          275,
          175,
          265,
          268,
          422,
          409,
          173,
          616,
          241,
          493,
          451,
          324,
          306,
          274,
          241,
          291,
          283,
          157,
          414,
          373,
          341,
          253,
          250,
          309,
          323,
          325,
          249,
          542,
          352,
          633,
          553,
          392,
          615,
          301,
          317,
          198,
          424,
          338,
          683,
          429,
          544,
          375,
          502,
          403,
          208,
          130,
          355,
          166,
          363,
          230,
          339,
          439,
          169,
          330,
          357,
          456,
          352,
          342,
          349,
          271,
          208,
          497,
          481,
          306,
          491,
          533,
          285,
          323,
          407,
          287,
          176,
          301,
          346,
          424,
          216,
          326,
          411,
          201,
          323,
          392,
          313,
          356,
          416,
          443,
          308,
          155,
          186,
          268,
          364,
          359,
          874,
          438,
          200,
          536,
          174,
          293,
          252,
          387,
          283,
          481,
          176,
          663,
          271,
          195,
          190,
          399,
          266,
          534,
          212,
          396,
          265,
          295,
          312,
          451,
          242,
          347,
          221,
          258,
          359,
          477,
          238,
          471,
          466,
          233,
          638,
          306,
          728,
          353,
          210,
          201,
          263,
          252,
          228,
          506,
          332,
          205,
          260,
          824,
          191,
          320,
          255,
          158,
          170,
          621,
          351,
          254,
          266,
          467,
          398,
          289,
          348,
          225,
          274,
          235,
          137,
          219,
          307,
          339,
          702,
          350,
          512,
          372,
          223,
          1320,
          205,
          319,
          211,
          291,
          240,
          572,
          226,
          346,
          475,
          254,
          701,
          462,
          324,
          195,
          303,
          402,
          337,
          515,
          475,
          221,
          379,
          223,
          197,
          440,
          424,
          307,
          710,
          315,
          219,
          464,
          758,
          445,
          469,
          295,
          414,
          348,
          318,
          330,
          626,
          91,
          223,
          219,
          127,
          204,
          753,
          221,
          270,
          320,
          328,
          366,
          352,
          160,
          252,
          239,
          301,
          834,
          923,
          267,
          268,
          239,
          134,
          87,
          301,
          523,
          660,
          266,
          211,
          554,
          330,
          441,
          262,
          258,
          227,
          149,
          118,
          252,
          127,
          335,
          415,
          362,
          260,
          223,
          123,
          193,
          294,
          295,
          428,
          253,
          305,
          478,
          335,
          252,
          316,
          356,
          460,
          328,
          544,
          233,
          525,
          452,
          297,
          303,
          595,
          359,
          231,
          430,
          324,
          169,
          303,
          344,
          389,
          187,
          303,
          436,
          195,
          250,
          289,
          327,
          818,
          448,
          304,
          548,
          236,
          471,
          521,
          391,
          298,
          701,
          245,
          260,
          172,
          133,
          307,
          204,
          542,
          256,
          533,
          250,
          334,
          269,
          371,
          228,
          282,
          323,
          580,
          462,
          492,
          380,
          222,
          281,
          367,
          463,
          364,
          240,
          274,
          364,
          298,
          270,
          217,
          317,
          555,
          332,
          1001,
          301,
          160,
          342,
          360,
          283,
          274,
          464,
          265,
          306,
          249,
          517,
          325,
          274,
          699,
          216,
          370,
          262,
          959,
          289,
          194,
          164,
          260,
          499,
          192,
          225,
          243,
          578,
          415,
          171,
          164,
          279,
          271,
          800,
          441,
          127,
          207,
          208,
          193,
          210,
          204,
          171,
          436,
          409,
          243,
          229,
          373,
          326,
          252,
          173,
          699,
          425,
          1339,
          370,
          540,
          198,
          284,
          506,
          331,
          229,
          408,
          1098,
          420,
          174,
          203,
          618,
          249,
          33,
          481,
          33,
          232,
          407,
          385,
          1302,
          233,
          447,
          310,
          459,
          183,
          136,
          319,
          230,
          396,
          234,
          337,
          757,
          913,
          301,
          271,
          317,
          138,
          225,
          228,
          163,
          666,
          293,
          691,
          622,
          342,
          283,
          182,
          176,
          451,
          410,
          743,
          449,
          446,
          276,
          414,
          517,
          336,
          301,
          243,
          215,
          176,
          277,
          342,
          199,
          229,
          257,
          543,
          813,
          369,
          300,
          302,
          184,
          219,
          326,
          204,
          229,
          703,
          262,
          409,
          315,
          293,
          622,
          561,
          384,
          306,
          159,
          316,
          196,
          401,
          626,
          537,
          677,
          417,
          249,
          237,
          462,
          307,
          288,
          356,
          303,
          399,
          798,
          303,
          376,
          533,
          173,
          212,
          1111,
          304,
          353,
          291,
          436,
          267,
          264,
          399,
          683,
          129,
          684,
          551,
          380,
          347,
          428,
          147,
          365,
          421,
          604,
          251,
          558,
          222,
          365,
          440,
          340,
          543,
          330,
          215,
          286,
          732,
          285,
          242,
          295,
          277,
          628,
          215,
          702,
          262,
          266,
          489,
          326,
          334,
          188,
          194,
          221,
          228,
          1144,
          288,
          683,
          225,
          415,
          380,
          206,
          388,
          331,
          416,
          258,
          194,
          275,
          665,
          333,
          272,
          439,
          211,
          463,
          299,
          543,
          361,
          322,
          270,
          159,
          291,
          363,
          320,
          302,
          327,
          460,
          168,
          417,
          433,
          247,
          345,
          232,
          141,
          915,
          361,
          208,
          287,
          298,
          245,
          271,
          355,
          229,
          413,
          290,
          371,
          314,
          559,
          510,
          287,
          298,
          311,
          695,
          35,
          341,
          465,
          181,
          525,
          373,
          483,
          190,
          439,
          173,
          353,
          425,
          248,
          234,
          153,
          193,
          223,
          121,
          284,
          232,
          242,
          566,
          290,
          710,
          428,
          224,
          180,
          311,
          434,
          304,
          627,
          461,
          192,
          310,
          183,
          371,
          534,
          326,
          239,
          345,
          473,
          663,
          202,
          229,
          310,
          231,
          201,
          570,
          369,
          377,
          217,
          444,
          516,
          222,
          257,
          226,
          556,
          285,
          230,
          330,
          448,
          274,
          377,
          867,
          374,
          188,
          333,
          227,
          283,
          359,
          342,
          311,
          254,
          466,
          745,
          524,
          210,
          311,
          891,
          253,
          383,
          390,
          410,
          205,
          326,
          121,
          177,
          305,
          101,
          328,
          402,
          642,
          307,
          196,
          274,
          427,
          353,
          868,
          307,
          386,
          383,
          977,
          125,
          390,
          403,
          415,
          643,
          256,
          181,
          286,
          408,
          529,
          380,
          170,
          154,
          252,
          341,
          216,
          258,
          501,
          246,
          272,
          522,
          250,
          265,
          225,
          272,
          417,
          256,
          601,
          545,
          591,
          335,
          181,
          205,
          206,
          223,
          212,
          387,
          415,
          263,
          264,
          318,
          37,
          418,
          594,
          221,
          213,
          340,
          615,
          363,
          235,
          318,
          656,
          151,
          197,
          282,
          416,
          525,
          283,
          613,
          257,
          343,
          178,
          291,
          416,
          256,
          446,
          355,
          267,
          328,
          226,
          298,
          127,
          492,
          172,
          272,
          329,
          221,
          417,
          289,
          436,
          372,
          374,
          468,
          317,
          536,
          138,
          263,
          450,
          339,
          1207,
          565,
          152,
          144,
          203,
          193,
          558,
          324,
          456,
          231,
          345,
          210,
          92,
          283,
          547,
          303,
          299,
          389,
          204,
          318,
          517,
          240,
          437,
          344,
          105,
          264,
          188,
          232,
          177,
          281,
          499,
          237,
          561,
          318,
          289,
          833,
          218,
          218,
          609,
          529,
          209,
          279,
          248,
          466,
          448,
          399,
          174,
          215,
          293,
          144,
          360,
          675,
          293,
          485,
          290,
          529,
          243,
          268,
          262,
          485,
          187,
          251,
          36,
          572,
          412,
          263,
          274,
          476,
          235,
          176,
          271,
          491,
          387,
          250,
          448,
          294,
          443,
          389,
          245,
          447,
          329,
          264,
          330,
          211,
          201,
          206,
          429,
          516,
          182,
          204,
          317,
          292,
          843,
          196,
          317,
          193,
          420,
          305,
          301,
          195,
          255,
          327,
          246,
          359,
          436,
          243,
          393,
          256,
          677,
          341,
          183,
          288,
          231,
          268,
          311,
          222,
          247,
          351,
          427,
          162,
          218,
          131,
          179,
          248,
          222,
          355,
          369,
          232,
          179,
          186,
          269,
          146,
          690,
          276,
          498,
          299,
          315,
          337,
          246,
          580,
          388,
          346,
          210,
          255,
          429,
          483,
          331,
          276,
          168,
          426,
          163,
          410,
          259,
          570,
          304,
          182,
          246,
          395,
          251,
          1145,
          650,
          250,
          272,
          570,
          375,
          250,
          690,
          259,
          345,
          105,
          118,
          314,
          182,
          258,
          253,
          229,
          498,
          203,
          502,
          553,
          329,
          358,
          313,
          453,
          160,
          320,
          245,
          161,
          281,
          579,
          228,
          395,
          460,
          151,
          652,
          174,
          322,
          258,
          414,
          419,
          216,
          531,
          370,
          263,
          276,
          380,
          703,
          396,
          772,
          22,
          20,
          21,
          501,
          337,
          278,
          1228,
          314,
          528,
          178,
          533,
          401,
          291,
          369,
          352,
          343,
          246,
          793,
          382,
          208,
          112,
          245,
          1071,
          629,
          614,
          241,
          210,
          150,
          417,
          364,
          244,
          211,
          193,
          139,
          540,
          180,
          168,
          128,
          407,
          326,
          224,
          206,
          289,
          671,
          262,
          235,
          269,
          510,
          383,
          449,
          309,
          628,
          190,
          382,
          367,
          263,
          307,
          362,
          261,
          152,
          447,
          294,
          761,
          227,
          313,
          275,
          303,
          365,
          295,
          159,
          320,
          555,
          367,
          71,
          629,
          629,
          495,
          370,
          377,
          311,
          368,
          170,
          536,
          508,
          376,
          171,
          399,
          178,
          216,
          282,
          457,
          543,
          130,
          642,
          318,
          139,
          240,
          347,
          337,
          268,
          418,
          638,
          484,
          498,
          231,
          162,
          190,
          489,
          338,
          178,
          462,
          309,
          264,
          179,
          207,
          347,
          299,
          478,
          301,
          122,
          221,
          453,
          314,
          187,
          269,
          366,
          533,
          449,
          426,
          268,
          306,
          470,
          292,
          263,
          201,
          211,
          395,
          297,
          391,
          355,
          316,
          381,
          228,
          276,
          398,
          283,
          322,
          529,
          313,
          238,
          348,
          236,
          339,
          466,
          609,
          202,
          262,
          413,
          309,
          73,
          114,
          343,
          178,
          366,
          769,
          160,
          382,
          581,
          1164,
          678,
          480,
          248,
          341,
          189,
          369,
          217,
          346,
          593,
          350,
          550,
          356,
          171,
          170,
          239,
          382,
          236,
          282,
          246,
          801,
          330,
          223,
          581,
          271,
          205,
          281,
          459,
          255,
          444,
          329,
          318,
          356,
          145,
          524,
          289,
          352,
          506,
          306,
          227,
          570,
          204,
          391,
          181,
          213,
          310,
          724,
          203,
          211,
          306,
          455,
          711,
          149,
          270,
          309,
          277,
          284,
          394,
          188,
          365,
          220,
          190,
          337,
          235,
          170,
          680,
          564,
          335,
          388,
          720,
          207,
          211,
          342,
          149,
          298,
          424,
          448,
          444,
          415,
          343,
          300,
          388,
          325,
          45,
          239,
          505,
          372,
          301,
          276,
          624,
          226,
          342,
          106,
          334,
          864,
          125,
          296,
          391,
          196,
          416,
          735,
          287,
          330,
          589,
          260,
          262,
          479,
          373,
          273,
          332,
          468,
          313,
          225,
          188,
          639,
          303,
          472,
          254,
          936,
          675,
          216,
          262,
          423,
          532,
          426,
          408,
          241,
          141,
          458,
          581,
          178,
          171,
          229,
          938,
          1390,
          662,
          402,
          391,
          228,
          573,
          577,
          484,
          180,
          419,
          404,
          2001,
          192,
          288,
          329,
          228,
          196,
          408,
          248,
          388,
          223,
          218,
          313,
          265,
          313,
          325,
          530,
          475,
          411,
          53,
          524,
          396,
          391,
          346,
          1013,
          667,
          374,
          485,
          268,
          127,
          396,
          491,
          282,
          314,
          478,
          293,
          246,
          468,
          493,
          275,
          260,
          510,
          236,
          368,
          304,
          262,
          217,
          330,
          210,
          321,
          366,
          308,
          243,
          202,
          400,
          236,
          247,
          373,
          257,
          344,
          473,
          357,
          192,
          211,
          383,
          357,
          176,
          233,
          198,
          219,
          420,
          174,
          741,
          288,
          253,
          209,
          457,
          343,
          217,
          208,
          196,
          153,
          199,
          218,
          566,
          360,
          182,
          286,
          223,
          282,
          194,
          204,
          469,
          162,
          464,
          502,
          262,
          277,
          480,
          542,
          352,
          283,
          264,
          207,
          148,
          196,
          161,
          364,
          821,
          425,
          206,
          232,
          504,
          248,
          221,
          651,
          359,
          316,
          319,
          169,
          360,
          251,
          207,
          114,
          94,
          217,
          666,
          225,
          284,
          352,
          202,
          241,
          256,
          181,
          264,
          239,
          200,
          342,
          392,
          462,
          628,
          241,
          175,
          158,
          558,
          122,
          217,
          273,
          256,
          405,
          221,
          274,
          618,
          147,
          393,
          294,
          467,
          332,
          338,
          334,
          194,
          234,
          426,
          227,
          300,
          316,
          368,
          282,
          387,
          926,
          245,
          448,
          499,
          120,
          407,
          310,
          248,
          456,
          157,
          201,
          224,
          281,
          371,
          432,
          265,
          278,
          323,
          444,
          268,
          414,
          333,
          280,
          482,
          496,
          420,
          345,
          751,
          623,
          260,
          227,
          361,
          395,
          446,
          486,
          565,
          145,
          154,
          495,
          235,
          296,
          279,
          585,
          249,
          316,
          238,
          207,
          215,
          231,
          246,
          537,
          212,
          402,
          495,
          902,
          209,
          670,
          263,
          341,
          298,
          291,
          342,
          172,
          366,
          295,
          282,
          286,
          285,
          328,
          335,
          326,
          550,
          484,
          665,
          144,
          198,
          429,
          475,
          319,
          200,
          371,
          407,
          190,
          468,
          203,
          768,
          328,
          701,
          315,
          191,
          210,
          255,
          383,
          208,
          220,
          318,
          136,
          171,
          1176,
          178,
          245,
          254,
          261,
          187,
          349,
          308,
          600,
          251,
          549,
          279,
          235,
          291,
          373,
          300,
          882,
          345,
          343,
          356,
          338,
          311,
          345,
          630,
          130,
          561,
          375,
          303,
          314,
          412,
          272,
          148,
          1159,
          391,
          643,
          760,
          324,
          362,
          146,
          423,
          225,
          253,
          428,
          417,
          486,
          269,
          482,
          308,
          440,
          429,
          301,
          251,
          262,
          377,
          223,
          168,
          228,
          366,
          430,
          180,
          435,
          366,
          407,
          1740,
          457,
          204,
          332,
          285,
          617,
          651,
          456,
          326,
          317,
          899,
          250,
          361,
          232,
          257,
          392,
          309,
          446,
          474,
          306,
          275,
          786,
          291,
          227,
          257,
          487,
          223,
          63,
          220,
          635,
          266,
          422,
          443,
          304,
          195,
          312,
          332,
          208,
          326,
          283,
          438,
          258,
          277,
          289,
          622,
          359,
          207,
          722,
          352,
          437,
          399,
          324,
          847,
          153,
          124,
          424,
          223,
          435,
          339,
          545,
          223,
          351,
          329,
          218,
          382,
          398,
          291,
          249,
          330,
          275,
          225,
          313,
          408,
          289,
          145,
          484,
          385,
          184,
          310,
          255,
          250,
          852,
          232,
          173,
          425,
          234,
          304,
          437,
          444,
          360,
          1280,
          284,
          332,
          367,
          696,
          395,
          300,
          315,
          250,
          190,
          260,
          357,
          360,
          278,
          300,
          339,
          209,
          422,
          443,
          575,
          295,
          219,
          325,
          359,
          388,
          224,
          207,
          574,
          315,
          515,
          209,
          337,
          326,
          263,
          333,
          174,
          173,
          216,
          260,
          129,
          778,
          176,
          184,
          334,
          383,
          246,
          438,
          341,
          187,
          553,
          377,
          347,
          398,
          354,
          187,
          731,
          210,
          201,
          372,
          336,
          328,
          281,
          418,
          341,
          724,
          209,
          166,
          165,
          396,
          674,
          440,
          484,
          354,
          403,
          256,
          291,
          440,
          264,
          143,
          562,
          593,
          524,
          333,
          584,
          514,
          333,
          172,
          546,
          436,
          372,
          394,
          370,
          388,
          456,
          134,
          277,
          260,
          372,
          2251,
          456,
          343,
          317,
          752,
          278,
          198,
          293,
          241,
          998,
          215,
          205,
          853,
          531,
          326,
          324,
          457,
          341,
          312,
          279,
          230,
          234,
          727,
          188,
          310,
          194,
          134,
          764,
          172,
          303,
          279,
          610,
          382,
          304,
          501,
          323,
          176,
          351,
          325,
          304,
          445,
          368,
          204,
          304,
          407,
          368,
          198,
          339,
          285,
          543,
          203,
          414,
          224,
          325,
          472,
          382,
          201,
          291,
          641,
          564,
          349,
          526,
          469,
          398,
          339,
          282,
          181,
          195,
          340,
          248,
          231,
          180,
          324,
          185,
          118,
          352,
          275,
          438,
          549,
          392,
          330,
          234,
          146,
          450,
          163,
          269,
          283,
          280,
          239,
          304,
          187,
          485,
          206,
          221,
          307,
          204,
          184,
          219,
          202,
          192,
          385,
          662,
          183,
          284,
          382,
          178,
          301,
          684,
          258,
          200,
          324,
          358,
          383,
          543,
          253,
          232,
          300,
          221,
          298,
          218,
          261,
          277,
          233,
          414,
          246,
          198,
          299,
          251,
          488,
          331,
          771,
          328,
          289,
          424,
          290,
          495,
          663,
          265,
          426,
          225,
          557,
          373,
          230,
          391,
          342,
          532,
          286,
          318,
          360,
          344,
          365,
          981,
          216,
          442,
          179,
          295,
          174,
          170,
          158,
          464,
          459,
          541,
          344,
          445,
          396,
          165,
          393,
          359,
          332,
          340,
          638,
          356,
          106,
          436,
          393,
          280,
          622,
          233,
          200,
          354,
          431,
          544,
          191,
          392,
          316,
          379,
          611,
          383,
          532,
          357,
          293,
          441,
          313,
          382,
          248,
          468,
          226,
          289,
          411,
          451,
          356,
          370,
          614,
          363,
          215,
          250,
          268,
          908,
          714,
          262,
          245,
          326,
          237,
          711,
          576,
          640,
          558,
          322,
          783,
          219,
          496,
          221,
          125,
          300,
          222,
          387,
          358,
          344,
          252,
          557,
          426,
          251,
          338,
          602,
          254,
          508,
          168,
          380,
          623,
          239,
          370,
          312,
          372,
          230,
          211,
          179,
          293,
          221,
          325,
          442,
          247,
          460,
          294,
          206,
          339,
          541,
          480,
          190,
          320,
          424,
          339,
          182,
          171,
          565,
          308,
          525,
          366,
          234,
          337,
          539,
          475,
          320,
          279,
          389,
          220,
          368,
          653,
          316,
          366,
          350,
          165,
          270,
          665,
          273,
          400,
          275,
          210,
          461,
          370,
          246,
          225,
          175,
          785,
          298,
          151,
          242,
          293,
          282,
          214,
          401,
          228,
          415,
          388,
          197,
          245,
          413,
          616,
          258,
          225,
          268,
          234,
          358,
          436,
          183,
          295,
          301,
          306,
          348,
          326,
          400,
          329,
          579,
          345,
          456,
          352,
          247,
          269,
          457,
          300,
          351,
          171,
          259,
          287,
          533,
          898,
          480,
          687,
          248,
          168,
          303,
          221,
          394,
          308,
          398,
          272,
          529,
          359,
          149,
          453,
          247,
          500,
          287,
          222,
          381,
          328,
          236,
          488,
          235,
          283,
          653,
          178,
          543,
          455,
          244,
          250,
          200,
          307,
          199,
          236,
          340,
          603,
          474,
          138,
          697,
          1181,
          481,
          338,
          628,
          176,
          558,
          203,
          262,
          560,
          187,
          227,
          267,
          235,
          150,
          308,
          226,
          613,
          566,
          621,
          291,
          233,
          214,
          252,
          166,
          431,
          248,
          249,
          164,
          236,
          413,
          594,
          522,
          428,
          533,
          275,
          371,
          360,
          382,
          165,
          393,
          299,
          210,
          277,
          528,
          279,
          297,
          241,
          589,
          173,
          508,
          369,
          273,
          139,
          235,
          614,
          289,
          494,
          343,
          220,
          637,
          553,
          363,
          297,
          207,
          283,
          347,
          372,
          463,
          303,
          266,
          232,
          438,
          711,
          408,
          161,
          610,
          363,
          284,
          274,
          337,
          418,
          319,
          179,
          252,
          225,
          303,
          195,
          277,
          236,
          177,
          363,
          265,
          450,
          594,
          258,
          630,
          250,
          458,
          293,
          368,
          600,
          592,
          161,
          202,
          454,
          722,
          206,
          233,
          322,
          259,
          448,
          255,
          592,
          387,
          316,
          482,
          393,
          320,
          244,
          343,
          357,
          529,
          389,
          322,
          415,
          299,
          747,
          216,
          566,
          312,
          272,
          419,
          379,
          554,
          620,
          344,
          497,
          407,
          564,
          540,
          284,
          417,
          208,
          412,
          130,
          504,
          187,
          255,
          256,
          220,
          507,
          252,
          201,
          219,
          217,
          774,
          610,
          561,
          248,
          268,
          458,
          521,
          1171,
          184,
          471,
          310,
          742,
          334,
          280,
          615,
          457,
          252,
          460,
          528,
          310,
          241,
          470,
          262,
          332,
          411,
          354,
          296,
          279,
          509,
          439,
          216,
          404,
          197,
          214,
          256,
          636,
          218,
          254,
          240,
          247,
          345,
          333,
          347,
          356,
          287,
          299,
          258,
          509,
          787,
          171,
          373,
          316,
          352,
          231,
          548,
          346,
          538,
          604,
          381,
          635,
          267,
          167,
          139,
          667,
          247,
          452,
          229,
          244,
          310,
          232,
          128,
          287,
          442,
          346,
          306,
          347,
          360,
          282,
          262,
          187,
          469,
          202,
          582,
          481,
          521,
          228,
          648,
          138,
          211,
          518,
          281,
          364,
          435,
          289,
          230,
          269,
          222,
          157,
          181,
          204,
          279,
          979,
          114,
          191,
          561,
          270,
          200,
          126,
          365,
          222,
          139,
          133,
          177,
          211,
          1082,
          287,
          340,
          632,
          247,
          227,
          493,
          318,
          163,
          128,
          188,
          464,
          285,
          534,
          315,
          358,
          216,
          174,
          177,
          412,
          307,
          420,
          418,
          166,
          563,
          685,
          532,
          618,
          427,
          258,
          128,
          273,
          517,
          312,
          293,
          259,
          207,
          131,
          353,
          162,
          299,
          306,
          386,
          399,
          507,
          204,
          284,
          340,
          233,
          994,
          256,
          183,
          216,
          147,
          267,
          240,
          482,
          343,
          221,
          194,
          375,
          261,
          341,
          201,
          221,
          261,
          458,
          583,
          315,
          262,
          225,
          430,
          178,
          177,
          164,
          709,
          277,
          292,
          836,
          246,
          621,
          402,
          877,
          147,
          299,
          300,
          207,
          248,
          307,
          301,
          376,
          329,
          204,
          229,
          235,
          736,
          233,
          315,
          308,
          289,
          371,
          156,
          600,
          248,
          368,
          280,
          188,
          959,
          522,
          581,
          378,
          352,
          659,
          179,
          32,
          244,
          295,
          182,
          228,
          239,
          606,
          303,
          457,
          1031,
          362,
          811,
          203,
          341,
          289,
          220,
          280,
          211,
          623,
          574,
          327,
          166,
          455,
          307,
          182,
          341,
          268,
          428,
          616,
          304,
          464,
          283,
          274,
          204,
          476,
          265,
          309,
          403,
          551,
          471,
          253,
          182,
          229,
          305,
          415,
          393,
          352,
          453,
          425,
          504,
          283,
          280,
          467,
          527,
          383,
          497,
          366,
          264,
          224,
          194,
          342,
          376,
          222,
          353,
          211,
          450,
          346,
          316,
          181,
          205,
          354,
          111,
          264,
          184,
          217,
          411,
          198,
          529,
          426,
          186,
          565,
          290,
          403,
          712,
          233,
          189,
          531,
          208,
          570,
          487,
          338,
          217,
          372,
          412,
          707,
          381,
          629,
          398,
          475,
          254,
          294,
          353,
          320,
          527,
          368,
          714,
          277,
          934,
          357,
          225,
          366,
          195,
          597,
          444,
          380,
          686,
          309,
          217,
          595,
          699,
          761,
          403,
          290,
          393
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Distribution of Word Count in text"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "text_word_count"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## checking the distribution of word count in text\n",
    "fig = px.histogram(data, x=\"text_word_count\", title=\"Distribution of Word Count in text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Word counts are fairly distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    print(\"flatten a list of lists to a combined list\")\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_chunk(texts):\n",
    "    nlp_list = list(nlp.pipe(texts, batch_size=100, n_process=100))\n",
    "    print(\"processing chunk..\")\n",
    "    return nlp_list\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=1000):\n",
    "    executor = Parallel(n_jobs=50, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(data), chunksize=chunksize))\n",
    "    print(\"Processing {} texts in {} jobs\".format(len(data), 50))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets create POS tags for each text and see the distribution of POS tags\n",
    "# docs = [nlp(text) for text in data['clean_text']]\n",
    "docs = [d for d in nlp.pipe(data['clean_text'], batch_size=100, n_process=100)]\n",
    "# docs = data[\"clean_text\"].apply(create_pos_tag)\n",
    "# docs = preprocess_parallel(data['clean_text'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating pos tags distribution\n",
    "token_distribution, is_alpha, is_stop = create_pos_tags_distribution(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the dictionary to a dataframe\n",
    "token_distribution_df = pd.DataFrame.from_dict(token_distribution, orient='index', columns=['count']).reset_index().rename(columns={\"index\": \"tags\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets create a distribution of POS tags\n",
    "## checking the distribution of word count in text\n",
    "fig = px.histogram(token_distribution_df, x=\"tags\", y=\"count\", text=\"Distribution POS Tags in text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets see how many words are alpha and how many are stop words\n",
    "print(f\"we have total {data['text_word_count'].sum()} words in the text. Out of which {is_alpha} are alpha and {is_stop} are stop words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So `maximum` tags are\n",
    "    * `PROPN`- proper noun\n",
    "    * `VERB` - verb\n",
    "    * `ADP` - adposition\n",
    "    * `NOUN` - noun\n",
    "    * `PUNCT` - punctuation\n",
    "    * `ADJ` - adjective\n",
    "* Since these are news article texts, I think useful tags are, \n",
    "    * `PROPN`\n",
    "    * `NOUN`\n",
    "    * `VERB`\n",
    "    * `ADJ` - Not sure about adjective yet. \n",
    "* We can remove rest of the words and still have a decent topic model. \n",
    "* We can also use the `is_stop` and `is_alpha` tags to remove the stopwords and non alpha tokens.\n",
    "    * Lets update the helper functions accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# lets see if we can calculate word frequency\n",
    "# all tokens that arent stop words or punctuations\n",
    "words = []\n",
    "for doc in docs:\n",
    "    doc_words = [token.text for token in doc if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "    words.append(doc_words)\n",
    "\n",
    "flat_list = [item for sublist in words for item in sublist]\n",
    "\n",
    "\n",
    "word_counts = Counter(flat_list)\n",
    "word_counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets try and plot the word cloud\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count']).reset_index().rename(columns={\"index\": \"word\"})\n",
    "\n",
    "fig = px.histogram(word_counts_df, x=\"word\", y=\"count\", text=\"Distribution POS Tags in text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Interesting the difference between `mean` and `max` frequency is quick big.  Wonder if that would cause issues in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets look at spacy merge entities \n",
    "# nlp.add_pipe(\"merge_noun_chunks\")\n",
    "merged_docs = [nlp(text) for text in data['clean_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_str = \"graham cruz tell mayorkas hes on notice for possible impeachment over border crisis\"\n",
    "\n",
    "# texts = [(t.lemma_, t.pos_) for t in merged_docs]\n",
    "# texts\n",
    "for merged_doc in merged_docs:\n",
    "    print([(\"_\".join(t.lemma_.split(\" \")), t.pos_) for t in merged_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* I think `merge_entities` and `merge_noun_entities` is what we want, it might end up with fewer words in vocab and we might not need bigrams/trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for merged_doc in merged_docs:\n",
    "    doc_words = [token.text for token in merged_doc if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "    words.append(doc_words)\n",
    "\n",
    "flat_list = [item for sublist in words for item in sublist]\n",
    "\n",
    "\n",
    "word_counts = Counter(flat_list)\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets try and plot the word cloud\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count']).reset_index().rename(columns={\"index\": \"word\"})\n",
    "\n",
    "fig = px.histogram(word_counts_df, x=\"word\", y=\"count\", text=\"Distribution POS Tags in text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Not too much difference in frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bigram & Tigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[10]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA with BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "## filter out words that occur less than 10 documents, or more than 75% of the documents.\n",
    "# id2word.filter_extremes(no_below=30, no_above=0.75, keep_n=10000)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=10,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                    #  chunksize=200,\n",
    "                    #  passes=250,\n",
    "                    #  alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA with Bigram BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with bigrams\n",
    "data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ'])\n",
    "data_lemmatized = lemmatization_noun_chunks(data_words_bigrams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=10,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=200,\n",
    "                     passes=200,\n",
    "                    #  alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So visually it seems we have a different topics when we use `bigrams`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA with Trigram BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with trigrams\n",
    "data_words_trigrams = make_trigrams(data_words, bigram_mod, trigram_mod)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=30,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=200,\n",
    "                     passes=200,\n",
    "                    #  alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LDA with Spacy noun-chunks n-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* In this case we are trying to use `Spacy's` noun-chunks to create n-grams.\n",
    "* We'll need to first create tokens from clean text. \n",
    "* We'll then need a function to replace the nouns in (noun chunks) with ngram word. \n",
    "* The `lemmatize` this, \n",
    "    * We'll need to test with and without POS to see if our filtering affects ngrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets start by reviewing noun chunks created by spacy\n",
    "docs = [create_pos_tag(\" \".join(x)) for x in data_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Noun Chunks Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = create_noun_chunk_ngrams(docs)\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization_without_pos(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So since we are doing `lemmatization` without any `POS` some parts of speech words are present in our lemmatized data. \n",
    "* Words like `doesn't`, `believe`, `think` etc are present. I think we should update the function to ignore certain `POS` rather than just include all words. \n",
    "* Lets do a quick check on how these `POS` are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_pos = [create_pos_tag(\" \".join(x)) for x in data_lemmatized]\n",
    "n_gram_pos_distribution, is_alpha, is_stop = create_pos_tags_distribution(n_gram_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_distribution_df = pd.DataFrame.from_dict(n_gram_pos_distribution, orient='index', columns=['count']).reset_index().rename(columns={\"index\": \"tags\"})\n",
    "## lets create a distribution of POS tags\n",
    "## checking the distribution of word count in text\n",
    "fig = px.histogram(token_distribution_df, x=\"tags\", y=\"count\", text=\"Distribution POS Tags in text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Lets see how our `ngrams` are tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in n_gram_pos:\n",
    "    for token in token:\n",
    "        ## Only print the noun chunks\n",
    "        if(\"_\" in token.text):            \n",
    "            print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Looks like most of the noun chunks are tagged as `NOUN`, `PROPN` or `ADJ`, but lets confirm it using visualization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks_tags = {}\n",
    "for token in n_gram_pos:\n",
    "    for token in token:\n",
    "        ## Only print the noun chunks\n",
    "        if(\"_\" in token.text):\n",
    "            ## increment the count of the noun chunk\n",
    "            noun_chunks_tags[token.pos_] = noun_chunks_tags.get(token.pos_, 0) + 1\n",
    "\n",
    "noun_chunks_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So as assumed most of them are `NOUN` or `PROPN` but just to make sure we don't loose any chunks, lets modify our lemmatization script to handle this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization_noun_chunks(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So the lemmatized data looks promising lets train the model and see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary & Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=30,\n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=200,\n",
    "                     passes=200,\n",
    "                    #  alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* So our model didn't perform well, in fact regular model BOW did better than noun chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn LDA with Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* For Count Vecotorization we'l use SKLearn's LDA algorithm. The algorithm is same as Gensim, but the interface is different and it allows us to use CountVectorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words)\n",
    "\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=35,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                      learning_decay=0.5\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())\n",
    "\n",
    "## \n",
    "# Log Likelihood:  -237441.44543701067\n",
    "# Perplexity:  1308.0281367579253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics = print_sklearn_dominant_topics(lda_model, data_vectorized)\n",
    "dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = print_sklearn_topic_distribution(lda_model, data_vectorized)\n",
    "topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Somehow I couldn't find out `coherence` for but from visualization it seems that the model has created topics which are easy to interpret and not overlapping\n",
    "* I still see some topics that don't make sense but, we can tweak this further by creating n-grams, including more POS and skipping lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_count_vectorization/\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn LDA with Bi-Grams Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with bigrams\n",
    "data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=33,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics = print_sklearn_dominant_topics(lda_model, data_vectorized)\n",
    "dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = print_sklearn_topic_distribution(lda_model, data_vectorized)\n",
    "topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_count_vectorization_bigrams/\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn LDA with noun-chunks n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets start by reviewing noun chunks created by spacy\n",
    "docs = [create_pos_tag(\" \".join(x)) for x in data_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Noun Chunks N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = create_noun_chunk_ngrams(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "# data_words_trigrams = make_trigrams(data_words, bigram_mod, trigram_mod)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "## performaing lemmatization with noun chunks to preserve the ngram words. \n",
    "# data_lemmatized = lemmatization_noun_chunks(n_grams)\n",
    "data_lemmatized = lemmatization(n_grams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=33,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics = print_sklearn_dominant_topics(lda_model, data_vectorized)\n",
    "dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = print_sklearn_topic_distribution(lda_model, data_vectorized)\n",
    "topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_count_vectorization_noun_chunks\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn with Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "data_words_trigrams = make_trigrams(data_words, bigram_mod, trigram_mod)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "## performaing lemmatization with noun chunks to preserve the ngram words. \n",
    "data_lemmatized = lemmatization_noun_chunks(data_words_trigrams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=33,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics = print_sklearn_dominant_topics(lda_model, data_vectorized)\n",
    "dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = print_sklearn_topic_distribution(lda_model, data_vectorized)\n",
    "topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_count_vectorization_trigrams\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn LDA with TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Earlier in the LDA we saw that there were a lot of terms with significantly higher frequency than the mean. \n",
    "* One theory is that these high frequency words might be biasing the topics, so we are going to use `TF-IDF` vectorization technique to see if we can fix that bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to create tfidf matrix\n",
    "def create_tfidf_matrix(data, max_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tfidf matrix\n",
    "data_vectorized, vectorizer = create_tfidf_matrix([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=33,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_tfidf_vectorization\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn LDA with Bigrams TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization with bigrams\n",
    "data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams)\n",
    "\n",
    "# Remove Stop Words\n",
    "## we are removing stop words in the lemmatization function using spacy is_stop flag\n",
    "# data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to create tfidf matrix\n",
    "def create_tfidf_matrix(data, max_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, max_df=0.95, stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tfidf matrix\n",
    "data_vectorized, vectorizer = create_tfidf_matrix([\" \".join(lem_word) for lem_word in data_lemmatized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build LDA Model 35\n",
    "lda_model = LatentDirichletAllocation(n_components=33,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=200,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sklearn_lda_model(lda_model, data_vectorized)\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vectorizer, lda_model, n_words=20\n",
    "df_topic_keywords = show_sklearn_topics(vectorizer, lda_model)\n",
    "formatted_topics = format_sklearn_topics(df_topic_keywords)\n",
    "formatted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = visualize_sklearn_lda_model(lda_model, data_vectorized, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "path = \"../pickles/sklearn_tfidf_bigram_vectorization\"\n",
    "\n",
    "with open(path + '/features_v1', 'wb') as files:\n",
    "    pickle.dump(features, files)\n",
    "    \n",
    "with open(path + '/model_v1', 'wb') as files:\n",
    "    pickle.dump(lda_model, files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Lets just do a quick test with the models that just saved to `pickle files` \n",
    "* In order to do that we need to take the texts through following steps, \n",
    "    * `Tokenize`\n",
    "    * `Create Noun Chunks`\n",
    "    * `Create Noun Chunks n-grams`\n",
    "    * `Lemmatization`\n",
    "    * `Count Vectorization`\n",
    "    * `LDA Transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load the model and features\n",
    "def load_pickle_files(path):\n",
    "    with open(path + '/model_v1', 'rb') as model:\n",
    "        lda = pickle.load(model)\n",
    "\n",
    "    # assuming you pickled the vectorizer\n",
    "    with open(path + '/features_v1', 'rb') as vocab:\n",
    "        features = pickle.load(vocab)\n",
    "    return (lda, features)\n",
    "\n",
    "\n",
    "def create_noun_chunks(data_words):\n",
    "    docs = [create_pos_tag(\" \".join(x)) for x in data_words]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../pickles/noun_chunks_model_v1', 'rb') as model:\n",
    "#     lda = pickle.load(model)\n",
    "\n",
    "# # assuming you pickled the vectorizer\n",
    "# with open('../pickles/noun_chunks_features_v1', 'rb') as vocab:\n",
    "#     features = pickle.load(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics(text, model, features):\n",
    "    # tokenize the text\n",
    "    # print(\"tokenizing the text...\")\n",
    "    data_words = list(sent_to_words(text))\n",
    "\n",
    "    # create noun chunks\n",
    "    # print(\"creating noun chunks...\")\n",
    "    docs = create_noun_chunks(data_words)\n",
    "\n",
    "    # create noun chunks ngrams\n",
    "    # print(\"creating noun chunk ngrams...\")\n",
    "    n_grams = create_noun_chunk_ngrams(docs)\n",
    "\n",
    "    # lemmatization\n",
    "    # print(\"lemmatization noun chunks...\")\n",
    "    data_lemmatized = lemmatization_noun_chunks(n_grams)\n",
    "\n",
    "    # count vectorization\n",
    "    # print(\"count vectorization...\")\n",
    "   \n",
    "    vectorizer = CountVectorizer(vocabulary=features)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # print(topicnames)\n",
    "    \n",
    "    data_vectorized = vectorizer.fit_transform([\" \".join(lem_word) for lem_word in data_lemmatized])\n",
    "\n",
    "    ## Create a dataframe with topics as rows and features as columns\n",
    "    ## each cell represents the weight of the feature in the topic\n",
    "    df_topic_keywords = pd.DataFrame(model.components_)\n",
    "    df_topic_keywords.columns = vectorizer.get_feature_names_out()\n",
    "    df_topic_keywords.index = topicnames  # type: ignore\n",
    "    \n",
    "    topic_keywords = show_sklearn_topics(vectorizer, model)\n",
    "    ## transform gives us the topic distribution for each document\n",
    "    ## here we have a list of probabilities for each topic, index of the list is the topic number\n",
    "    topic_probability_scores = model.transform(data_vectorized)\n",
    "    \n",
    "    ## from the dataframe we then select the row with the highest probability\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()  # type: ignore\n",
    "    topic_list = list(topic_keywords)[np.argmax(topic_probability_scores)]\n",
    "    topic.sort(reverse=True)    \n",
    "    features_with_weights = list(zip(topic_list, topic))\n",
    "    return features_with_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_text = data.sample(1)[\"clean_text\"].tolist()\n",
    "random_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing SKLearn Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_count_vectorization, features_count_vectorization) = load_pickle_files(\"../pickles/sklearn_count_vectorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import time\n",
    "# for text in random_text:\n",
    "#     print(text)\n",
    "#     features_with_weights = predict_topics(text, lda_count_vectorization, features_count_vectorization)\n",
    "#     print(features_with_weights)\n",
    "#     print(\" \")\n",
    "\n",
    "features_with_weights = predict_topics(random_text, lda_count_vectorization, features_count_vectorization)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing Count Vectorization With BiGrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_with_bigrams, features_with_bigrams) = load_pickle_files(\"../pickles/sklearn_count_vectorization_bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_weights = predict_topics(random_text, lda_with_bigrams, features_with_bigrams)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing Count Vectorization With Tri-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_with_trigrams, features_with_trigrams) = load_pickle_files(\"../pickles/sklearn_count_vectorization_trigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_weights = predict_topics(random_text, lda_with_trigrams, features_with_trigrams)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing Count Vectorization With Noun Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_with_noun_chunks, features_with_noun_chunks) = load_pickle_files(\"../pickles/sklearn_count_vectorization_noun_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_weights = predict_topics(random_text, lda_with_noun_chunks, features_with_noun_chunks)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_tf_idf, features_tf_idf) = load_pickle_files(\"../pickles/sklearn_tfidf_vectorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_weights = predict_topics(random_text, lda_tf_idf, features_tf_idf)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Testing Bigram TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lda_tf_idf, features_tf_idf) = load_pickle_files(\"../pickles/sklearn_tfidf_bigram_vectorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_weights = predict_topics(random_text, lda_tf_idf, features_tf_idf)\n",
    "print(random_text)\n",
    "print(features_with_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "914eef65da1375b54976b1eb4c0a6192cf68c9b736369c9231d1c03e2f3fef86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
