{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scrapping\n",
    "* In this notebook we'll scrape news articles from `Fox` and `CNN` for further analysis. \n",
    "* In order focus on the functionality we'll limit the scope of news to the `politics` section of the website. \n",
    "* Running this notebook should create atleast 2 `CSV` files with data from both news source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/vscode/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "\u001b[33mWARNING: The directory '/home/vscode/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: fake_useragent in /opt/conda/lib/python3.9/site-packages (0.1.11)\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4\n",
    "! pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting User-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# setting up the user agent\n",
    "ua = UserAgent()\n",
    "\n",
    "## setting up the headers\n",
    "headers = {'User-Agent': ua.chrome}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Fox News\n",
    "\n",
    "### Notes\n",
    "* After exploring the `foxnews.com` website, we've found that `get` call on following URL gives us the list of news articles `metadata` and actual article URL. \n",
    "\n",
    "```\n",
    "https://www.foxnews.com/api/article-search?searchBy=categories&values=fox-news%2Fpolitics&size=11&from=1&mediaTags=primary_politics\n",
    "```\n",
    "* In above URL request param `size` determines the number of results returned and `from` determines starting page number. \n",
    "* From the given `metadata` we'll filter out the articles which are just `VIDEO` articles and create a data-set.\n",
    "* We'll loop thru the `metadata`do a get call actual article URL scrape and record the article contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['imageUrl', 'title', 'description', 'url', 'publicationDate', 'lastPublishedDate', 'category', 'isBreaking', 'duration', 'authors'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setting default param values. \n",
    "fromParam = 1\n",
    "sizeParam = 10\n",
    "\n",
    "data = requests.get(\"https://www.foxnews.com/api/article-search?searchBy=categories&values=fox-news%2Fpolitics&size=11&from=1&mediaTags=primary_politics\")\n",
    "json_data = data.json()\n",
    "json_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
